"""
RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –º–µ—Ç–æ–¥–∏–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∏—Å–∫–∞–º–∏.
"""

import logging
import json
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

import chromadb
from chromadb.config import Settings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

from config.settings import settings


class KnowledgeBase:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ RAG."""
    
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.text_splitter = None
        self.documents: List[Document] = []
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
        try:
            logging.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π RAG...")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embeddings
            self.embeddings = HuggingFaceEmbeddings(
                model_name=settings.embedding_model,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è text splitter
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.chunk_size,
                chunk_overlap=settings.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", ". ", "! ", "? ", " "]
            )
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ JSONL —Ñ–∞–π–ª–∞
            await self._load_documents_from_jsonl()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            await self._initialize_vectorstore()
            
            logging.info("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            raise
    
    async def _load_documents_from_jsonl(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ JSONL —Ñ–∞–π–ª–∞."""
        try:
            jsonl_path = Path("ai_agent/rag/data/methodology.jsonl")
            
            if not jsonl_path.exists():
                logging.warning(f"‚ö†Ô∏è –§–∞–π–ª {jsonl_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return
            
            documents = []
            
            with open(jsonl_path, 'r', encoding='utf-8') as file:
                for line_num, line in enumerate(file, 1):
                    try:
                        data = json.loads(line.strip())
                        
                        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ prompt-response –ø–∞—Ä—ã
                        prompt = data.get('prompt', '')
                        response = data.get('response', '')
                        metadata = data.get('metadata', {})
                        
                        if prompt and response:
                            # –î–æ–∫—É–º–µ–Ω—Ç –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
                            doc_question = Document(
                                page_content=f"–í–æ–ø—Ä–æ—Å: {prompt}",
                                metadata={
                                    **metadata,
                                    "type": "question",
                                    "source": "methodology_jsonl",
                                    "line_number": line_num
                                }
                            )
                            documents.append(doc_question)
                            
                            # –î–æ–∫—É–º–µ–Ω—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
                            doc_answer = Document(
                                page_content=f"–û—Ç–≤–µ—Ç: {response}",
                                metadata={
                                    **metadata,
                                    "type": "answer", 
                                    "source": "methodology_jsonl",
                                    "line_number": line_num,
                                    "related_question": prompt
                                }
                            )
                            documents.append(doc_answer)
                            
                            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                            combined_content = f"{prompt}\n\n{response}"
                            doc_combined = Document(
                                page_content=combined_content,
                                metadata={
                                    **metadata,
                                    "type": "qa_pair",
                                    "source": "methodology_jsonl",
                                    "line_number": line_num
                                }
                            )
                            documents.append(doc_combined)
                    
                    except json.JSONDecodeError as e:
                        logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {line_num}: {e}")
                        continue
            
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏
            all_chunks = []
            for doc in documents:
                chunks = self.text_splitter.split_documents([doc])
                all_chunks.extend(chunks)
            
            self.documents = all_chunks
            logging.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            raise
    
    async def _initialize_vectorstore(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            vectorstore_path = Path(settings.vector_store_path)
            vectorstore_path.mkdir(parents=True, exist_ok=True)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            if self._vectorstore_exists():
                logging.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
                self.vectorstore = Chroma(
                    persist_directory=str(vectorstore_path),
                    embedding_function=self.embeddings,
                    collection_name="bank_risk_methodology"
                )
            else:
                logging.info("üî® –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
                
                if not self.documents:
                    logging.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
                    return
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
                self.vectorstore = Chroma.from_documents(
                    documents=self.documents,
                    embedding=self.embeddings,
                    persist_directory=str(vectorstore_path),
                    collection_name="bank_risk_methodology"
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
                self.vectorstore.persist()
                logging.info(f"üíæ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
            raise
    
    def _vectorstore_exists(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        vectorstore_path = Path(settings.vector_store_path)
        return (vectorstore_path / "chroma.sqlite3").exists()
    
    async def search(self, query: str, limit: int = 5, filter_metadata: Dict = None) -> List[Document]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        try:
            if not self.vectorstore:
                logging.warning("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
                return []
            
            # –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
            search_kwargs = {"k": limit}
            if filter_metadata:
                search_kwargs["filter"] = filter_metadata
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            results = self.vectorstore.similarity_search(query, **search_kwargs)
            
            logging.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
            
            return results
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def search_with_scores(self, query: str, limit: int = 5) -> List[tuple]:
        """–ü–æ–∏—Å–∫ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏."""
        try:
            if not self.vectorstore:
                return []
            
            results = self.vectorstore.similarity_search_with_score(query, k=limit)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            filtered_results = [(doc, score) for doc, score in results if score < 0.8]  # –ú–µ–Ω—å—à–µ = –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
            
            logging.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(filtered_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            return filtered_results
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å –æ—Ü–µ–Ω–∫–∞–º–∏: {e}")
            return []
    
    async def get_context_for_question(self, question: str, difficulty: int = None) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å."""
        try:
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            filter_metadata = {}
            if difficulty:
                filter_metadata["difficulty"] = difficulty
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_docs = await self.search(question, limit=3, filter_metadata=filter_metadata)
            
            if not relevant_docs:
                # –ü–æ–ø—ã—Ç–∫–∞ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                relevant_docs = await self.search(question, limit=3)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_parts = []
            for doc in relevant_docs:
                content = doc.page_content
                metadata = doc.metadata
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                source_info = f"–ò—Å—Ç–æ—á–Ω–∏–∫: {metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
                if metadata.get('topic'):
                    source_info += f", –¢–µ–º–∞: {metadata['topic']}"
                if metadata.get('difficulty'):
                    source_info += f", –£—Ä–æ–≤–µ–Ω—å: {metadata['difficulty']}"
                
                context_parts.append(f"{content}\n({source_info})")
            
            context = "\n\n---\n\n".join(context_parts)
            
            logging.info(f"üìù –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return context
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return ""
    
    async def add_document(self, content: str, metadata: Dict = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π."""
        try:
            if not self.vectorstore:
                logging.warning("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
                return
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc = Document(
                page_content=content,
                metadata=metadata or {}
            )
            
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.text_splitter.split_documents([doc])
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            self.vectorstore.add_documents(chunks)
            self.vectorstore.persist()
            
            logging.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
    
    async def update_knowledge_base(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ JSONL —Ñ–∞–π–ª–∞."""
        try:
            logging.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
            
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—É—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self.documents = []
            
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            await self._load_documents_from_jsonl()
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            if self.documents:
                vectorstore_path = Path(settings.vector_store_path)
                
                # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
                import shutil
                if vectorstore_path.exists():
                    shutil.rmtree(vectorstore_path)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ
                await self._initialize_vectorstore()
                
                logging.info("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
    
    async def search_by_topic(self, topic: str, limit: int = 10) -> List[Document]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–µ."""
        try:
            filter_metadata = {"topic": topic}
            results = await self.search("", limit=limit, filter_metadata=filter_metadata)
            return results
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–µ–º–µ: {e}")
            return []
    
    async def search_by_difficulty(self, difficulty: int, limit: int = 10) -> List[Document]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏."""
        try:
            filter_metadata = {"difficulty": difficulty}
            results = await self.search("", limit=limit, filter_metadata=filter_metadata)
            return results
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {e}")
            return []
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
        try:
            if not self.vectorstore:
                return {"error": "–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ"}
            
            # –ü–æ–¥—Å—á–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–∏–ø–∞–º –∏ —Ç–µ–º–∞–º
            stats = {
                "total_documents": len(self.documents),
                "by_type": {},
                "by_topic": {},
                "by_difficulty": {}
            }
            
            for doc in self.documents:
                metadata = doc.metadata
                
                # –ü–æ —Ç–∏–ø—É
                doc_type = metadata.get("type", "unknown")
                stats["by_type"][doc_type] = stats["by_type"].get(doc_type, 0) + 1
                
                # –ü–æ —Ç–µ–º–µ
                topic = metadata.get("topic", "unknown")
                stats["by_topic"][topic] = stats["by_topic"].get(topic, 0) + 1
                
                # –ü–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                difficulty = metadata.get("difficulty", "unknown")
                stats["by_difficulty"][difficulty] = stats["by_difficulty"].get(difficulty, 0) + 1
            
            return stats
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {"error": str(e)}
    
    async def get_related_questions(self, question: str, limit: int = 5) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤."""
        try:
            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            filter_metadata = {"type": "question"}
            results = await self.search(question, limit=limit, filter_metadata=filter_metadata)
            
            related_questions = []
            for doc in results:
                content = doc.page_content
                if content.startswith("–í–æ–ø—Ä–æ—Å: "):
                    question_text = content[8:]  # –£–±–∏—Ä–∞–µ–º "–í–æ–ø—Ä–æ—Å: "
                    if question_text not in related_questions and question_text != question:
                        related_questions.append(question_text)
            
            return related_questions[:limit]
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
            return []
    
    async def get_random_questions_by_difficulty(self, difficulty: int, count: int = 3) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏."""
        try:
            import random
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω—É–∂–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            filter_metadata = {"type": "qa_pair", "difficulty": difficulty}
            all_questions = await self.search("", limit=50, filter_metadata=filter_metadata)
            
            if not all_questions:
                return []
            
            # –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä
            selected = random.sample(all_questions, min(count, len(all_questions)))
            
            questions = []
            for doc in selected:
                content = doc.page_content
                if "\n\n" in content:
                    question_part, answer_part = content.split("\n\n", 1)
                    questions.append({
                        "question": question_part,
                        "answer": answer_part,
                        "metadata": doc.metadata
                    })
            
            return questions
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
            return []


class QueryProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
    
    def __init__(self, knowledge_base: KnowledgeBase):
        self.knowledge_base = knowledge_base
        self.query_cache = {}  # –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à –¥–ª—è —á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    
    async def process_query(self, query: str, user_context: Dict = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
            normalized_query = self._normalize_query(query)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            if normalized_query in self.query_cache:
                logging.info(f"üí® –û—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:30]}...")
                return self.query_cache[normalized_query]
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
            query_type = self._classify_query(query)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            user_difficulty = user_context.get("difficulty", 1) if user_context else 1
            context = await self.knowledge_base.get_context_for_question(query, user_difficulty)
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_docs = await self.knowledge_base.search(query, limit=5)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = {
                "query": query,
                "normalized_query": normalized_query,
                "query_type": query_type,
                "context": context,
                "relevant_documents": [
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata
                    } for doc in relevant_docs
                ],
                "confidence": self._calculate_confidence(relevant_docs),
                "suggestions": await self.knowledge_base.get_related_questions(query, 3)
            }
            
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            self.query_cache[normalized_query] = result
            
            return result
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return {"error": str(e)}
    
    def _normalize_query(self, query: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞."""
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        normalized = query.lower().strip()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        normalized = " ".join(normalized.split())
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
        normalized = normalized.rstrip(".,!?;:")
        
        return normalized
    
    def _classify_query(self, query: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞."""
        query_lower = query.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if any(word in query_lower for word in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"]):
            return "definition"
        elif any(word in query_lower for word in ["–∫–∞–∫", "–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "—Å–ø–æ—Å–æ–±"]):
            return "instruction"
        elif any(word in query_lower for word in ["–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–ø—Ä–∏—á–∏–Ω–∞"]):
            return "explanation"
        elif any(word in query_lower for word in ["—Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å", "—Ñ–æ—Ä–º—É–ª–∞", "—Ä–∞—Å—á–µ—Ç"]):
            return "calculation"
        elif any(word in query_lower for word in ["–ø—Ä–∏–º–µ—Ä", "—Å—Ü–µ–Ω–∞—Ä–∏–π", "—Å–ª—É—á–∞–π"]):
            return "example"
        elif query_lower.endswith("?"):
            return "question"
        else:
            return "general"
    
    def _calculate_confidence(self, documents: List[Document]) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        if not documents:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        confidence = min(len(documents) / 5.0, 1.0)
        
        # –ë–æ–Ω—É—Å –∑–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        for doc in documents:
            if doc.metadata.get("type") == "qa_pair":
                confidence += 0.1
            if doc.metadata.get("source") == "methodology_jsonl":
                confidence += 0.05
        
        return min(confidence, 1.0)
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∑–∞–ø—Ä–æ—Å–æ–≤."""
        self.query_cache.clear()
        logging.info("üóëÔ∏è –ö—ç—à –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—á–∏—â–µ–Ω")


# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å JSONL
class JSONLProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSONL —Ñ–∞–π–ª–æ–≤."""
    
    @staticmethod
    def validate_jsonl_file(file_path: str) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è JSONL —Ñ–∞–π–ª–∞."""
        try:
            stats = {
                "total_lines": 0,
                "valid_lines": 0,
                "invalid_lines": 0,
                "errors": []
            }
            
            with open(file_path, 'r', encoding='utf-8') as file:
                for line_num, line in enumerate(file, 1):
                    stats["total_lines"] += 1
                    
                    try:
                        data = json.loads(line.strip())
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
                        if "prompt" in data and "response" in data:
                            stats["valid_lines"] += 1
                        else:
                            stats["invalid_lines"] += 1
                            stats["errors"].append(f"–°—Ç—Ä–æ–∫–∞ {line_num}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è")
                    
                    except json.JSONDecodeError as e:
                        stats["invalid_lines"] += 1
                        stats["errors"].append(f"–°—Ç—Ä–æ–∫–∞ {line_num}: –æ—à–∏–±–∫–∞ JSON - {e}")
            
            return stats
            
        except Exception as e:
            return {"error": str(e)}
    
    @staticmethod
    def convert_to_jsonl(input_data: List[Dict], output_path: str):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ JSONL —Ñ–æ—Ä–º–∞—Ç."""
        try:
            with open(output_path, 'w', encoding='utf-8') as file:
                for item in input_data:
                    json_line = json.dumps(item, ensure_ascii=False)
                    file.write(json_line + '\n')
            
            logging.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(input_data)} –∑–∞–ø–∏—Å–µ–π –≤ {output_path}")
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSONL: {e}")
            raise